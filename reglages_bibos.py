from minimiseur_bibos import *

reglages_nn = {'n_tours': 2, 'nombre de rouleaux': 10, 'taille des batchs': 32, 'nombre de couches': 150}
reglages_par_defaut = {
 # ParamÃ¨tres d'entrÃ©e :
  'nombre de rouleaux' : 2,

 # RÃ‰GLAGES INTERNES AU RÃ‰SEAU DE NEURONES : 

  'nombre de couches': 15, 
  # Plus c'est gros, plus on pourra faire avoir des rÃ©sultats prÃ©cis mais plus il faudra entraÃ®ner

  'n_tours' : 100,

  'nombre explorateurs' : 1, 
  #Plus c'est grand, plus on explore l'espace mais plus on doit faire de calculs

  'taille des batchs' : 32, 
  # Plus c'est grand, meilleure sera l'idÃ©e de la direction Ã  explorer mais plus grand sera le temps de calcul

  # ğŸŒ¼ğŸŒ¼ğŸŒ¼ Nombre d'Ã©chantillons par entrÃ©es
  'echantillons par entree' : 4,
  # Ã  augmenter si le rÃ©seau semble avoir du mal Ã  comprendre dans quelle direction Ã©voluer. Plus c'est grand, plus c'est lent 

  'taille couches intermediaires' : 64, 
  # Plus c'est gros, plus on pourra faire avoir des rÃ©sultats prÃ©cis mais plus il faudra entraÃ®ner

  'portee explorateurs' : 0.05, 
  #Distance en dessous de laquelle les explorateurs sont considÃ©rÃ©s trop proches

  'reactivite prudence': 1/3 , 
  #proche de 1 => trÃ¨s rÃ©actif, proche de 0 => trÃ¨s lent Ã  rÃ©agir pour ajuster la variance exploratoire en fonction de la variance observÃ©e des rÃ©compenses 

  "seuil prudence" : 0.5, 
  # variation en ordre de grandeur acceptÃ© sur la rÃ©compense lors du tirage depuis un mÃªme mode

  'variance initiale':0.01, 
  # Vitesse d'exploration initiale

  'seuil exploration minimal' :1/3, 
  # On dÃ©die cette proportion des calculs aux explorateurs qui ont encore rien dÃ©couvert d'interressant 

  # ğŸŒ¼ğŸŒ¼ğŸŒ¼ Variance d'exploration initiale lorsque l'on fait de l'annealing
  'variance annealing debut' : 0.01,
  # Si la stratÃ©gie semble converger trop lentement (ie que la rÃ©compense est croissante et ne semble pas stagner aprÃ¨s beaucoup d'itÃ©rations), on peut tenter d'augmenter. Si n'arrive pas Ã  avoir une rÃ©compense qui augmente, on peut tenter de diminuer

  # ğŸŒ¼ğŸŒ¼ğŸŒ¼ Variance d'exploration finale lorsque l'on fait de l'annealing
  'variance annealing fin' : 0.0001,
  #Si la rÃ©compense oscille lorsqu'elle est stabilisÃ©e, c'est que la variance est trop grande. On peut alors tenter de diminuer cette valeur. Si la stratÃ©gie semble "bloquÃ©e" sur la fin (dÃ©rivÃ©e de la rÃ©compense par rapport Ã  la taille du pas est non presque nulle, mais la stratÃ©gie bouge peu), alors on peut tenter d'augmenter.

  # ğŸŒ¼ğŸŒ¼ğŸŒ¼ Modification de la variance d'exploration pour rÃ©duire la variance des rÃ©compense Ã  l'interieur d'un batch
  'Controlabilite VIB' : False,
  # Si on a un seul explorateur, est pertinent d'utilisation. Sinon, on risque d'avoir des prolÃ¨mes. Peut Ã©ventuellement Ãªtre utilisÃ© si le nombre d'explorateur <= 2* echantillons par entree. 

  'max variance exploration autorisee' : 1, 
  #Vitesse d'exploration maximale autorisÃ©e

  'max acceleration variance exploration' : 2,
   # Plus c'est grand, plus c'est instable. Plus c'est petit, plus on prendre du temps Ã  converger alors qu'on aurait pÃ» aller plus vite

  'min variance exploration autorisee' : 1e-8, 
  #Vitesse d'exploration minimale autorisÃ©e. Sera atteinte lorsque l'on sera dans un optimum local

  'memoire variation recompenses' : 4 ,
   #Nombre des derniers tours que l'on considÃ¨re pour savoir comment diminuer le learning rate de l'optimiseur. Si trop gros, risque d'Ãªtre trop lent Ã  rÃ©agir. Si trop rapide, sera en mode Dory et oubliera qu'il doit Ãªtre prudent Ã  l'instant oÃ¹ le danger ne sera plus en vue (mais sera encore lÃ )

  'acceleration max learning rate' : 1.5, 
  #Plus c'est grand, plus on risque d'Ãªtre instable. Plus c'est petit, plus on prend du temps Ã  converger alors qu'on aurait pu aller plus vite. Le diminuer permet de diminuer la memoire de la variation des recompenses.

  # ğŸŒ¼ğŸŒ¼ğŸŒ¼  Learning rate modifiÃ© en forcing ?
  'Interventionnisme learning rate' : False,
  # Si la variance entre rÃ©compense est Ã©norme d'une itÃ©ration sur l'autre, que les batchs sont de taille Ã©norme et que la rÃ©compense n'augmente pas aprÃ¨s beaucoup d'itÃ©rations, on peut essayer de le mettre en True

  # ğŸŒ¼ğŸŒ¼ğŸŒ¼  Learning rate minimal acceptÃ©
  'learning rate minimal' : 0,


  # ğŸŒ¼ğŸŒ¼ğŸŒ¼ Learning rate maximal acceptÃ©
  'learning rate maximal' : 0.001,
  # Si trop Ã©levÃ©e, on risque, les fois oÃ¹ la commandabilitÃ© ne suffira pas, de se dÃ©placer trÃ¨s fortement dans le monde des paramÃ¨tres. Si il est trop faible, on va vraiment avoir du mal Ã  converger
  

 # ğŸŒ¼ğŸŒ¼ğŸŒ¼ DiffÃ©rence maximale de rÃ©compense acceptÃ©e entre deux itÃ©rations successives
 'variance recompense acceptee' : 1,
 # Plus c'est  grand, plus on ira vite vers un optimum mais plus on risque d'Ãªtre instable et de ne pas converger. Plus c'est petit, plus on prend du temps Ã  converger alors qu'on aurait pu potentiellement aller plus vite. 
 # Signe qu'il faut l'abaisser : la courbe de rÃ©compenses fluctue beaucoup

  'reglages minimisation' : {
    'resol_main' : resol_a_la_main_bibos
                             },
# ğŸŒ¼ğŸŒ¼ğŸŒ¼ Choisit la fonction Ã  Ã©valuer pour obtenir la rÃ©compense
  'fonction a evaluer' : 'version normale',






 #Sauvegarde et chargement des paramÃ¨tres du reseau entraÃ®nÃ© :
  'path vers la base de donnees des reseaux' : '/Workspace/Users/theo.duquesne@armor-iimak.com/empilements des cylindres2/bases de donnÃ©es/Base_Donnees_Reseaux.json',
  'path parametres defaut' : '/Workspace/Users/theo.duquesne@armor-iimak.com/empilements des cylindres2/modules/parametres_rdn/',






  # ParamÃ¨tres des indicateurs

  # ğŸŒ¼ğŸŒ¼ğŸŒ¼ Afficher la dÃ©viation de la stratÃ©gie ?
  'calculer deviation strategie' : False,
  # Ã§a demande un peu de temps de calcul en plus, mais c'est un indicateur intÃ©ressant

  # ğŸŒ¼ğŸŒ¼ğŸŒ¼ Afficher la vitesse de la stratÃ©gie ?
  'calculer vitesse strategie' : False
  # Ã§a demande un peu de temps de calcul en plus, mais c'est un indicateur intÃ©ressant



}